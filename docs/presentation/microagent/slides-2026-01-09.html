<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Zach Morek">
  <title>Microagent</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^5/dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^5/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^5/dist/theme/white.css" id="theme">
  <style>
    .reveal { font-size: 28px; }
    .reveal h1 { font-size: 1.8em; text-align: center; }
    .reveal h2 { font-size: 1.4em; text-align: center; }
    .reveal pre { font-size: 0.7em; }
    .reveal code { font-size: 0.85em; background: rgba(0,0,0,0.08); padding: 0.1em 0.3em; border-radius: 3px; }
    .reveal table { font-size: 0.8em; }
    .smaller { font-size: 0.8em; }
    .reveal .slides section { text-align: left; }
    .reveal .slides #title-slide { text-align: center; }
    .reveal .slides section ul,
    .reveal .slides section ol {
      display: block;
      margin-left: 1em;
    }
    .centered { text-align: center; }
  </style>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Microagent</h1>
  <p class="subtitle">Daily logging meets eval-driven development</p>
  <p class="author">Zach Morek</p>
  <p class="date">Jan 9, 2025</p>
</section>

<section id="claude-as-coach-works-but-its-locked-to-claude"
class="slide level1">
<h1>Claude-as-coach works, but it’s locked to Claude</h1>
<p><a
href="https://github.com/ZachBeta/claude-as-coach">Claude-as-coach</a>
is effective. But platform lock-in limits what we can do.</p>
<p><strong>Platform lock-in:</strong></p>
<ul>
<li>Pro subscription required (Max preferred) — hard to share with
others</li>
<li>Locked to haiku/sonnet/opus — all proprietary models</li>
<li>No model comparison possible</li>
</ul>
<p><strong>Manual processes:</strong></p>
<ul>
<li>Clunky to install skill bundles as an “app” that runs on claude.ai
project “platform”</li>
<li>Lots of manual context curation</li>
<li>File backup painful: download artifact → move to repo → commit →
remove</li>
</ul>
<p><strong>Portability:</strong></p>
<ul>
<li>Skills are useful but need adaptation for other platforms</li>
</ul>
<p>What if we wanted the same workflow with any tool-calling model?</p>
</section>
<section id="we-need-the-same-workflow-with-more-ownership"
class="slide level1">
<h1>We need the same workflow with more ownership</h1>
<p>Build a <strong>model-agnostic</strong> daily logging and reflection
assistant.</p>
<p><em>Prompt-native knowledge management. Multishot learning through
context curation.</em></p>
<p><strong>What I wanted:</strong></p>
<ul>
<li>Same workflow as claude-as-coach</li>
<li>Any tool-calling model (not just Claude)</li>
<li>Local conversation logging</li>
<li>Eval infrastructure to compare models</li>
</ul>
<p><strong>What I built:</strong> Microagent (1546 lines) + eval_runner
(836 lines)</p>
</section>
<section id="evolution-claude-projects-local-agent"
class="slide level1">
<h1>Evolution: Claude Projects → Local Agent</h1>
<h2 id="the-journey-from-platform-to-portable">The journey from platform
to portable</h2>
</section>
<section id="claude-projects-let-us-prototype-easily"
class="slide level1">
<h1>Claude Projects let us prototype easily</h1>
<p>Claude-as-coach: Skills + Project Documents</p>
<ul>
<li>Works great for daily logging</li>
<li>Context management via fractal compression</li>
<li>But: locked to Claude.ai</li>
</ul>
<p><em>Great inference quality/$ but friction to share with
others.</em></p>
<p><a
href="https://zachbeta.github.io/claude-as-coach/slides-2025-12-12.html">Claude
As Coach AIIA Presentation 2025-12-12</a></p>
</section>
<section id="claude-projects-keep-small-files-always-in-context"
class="slide level1">
<h1>Claude Projects keep small files always in context</h1>
<p>Claude.ai’s project feature: persistent documents across
conversations.</p>
<p><strong>Key insight:</strong> Small files are always in context (no
RAG needed at this scale) - “context packing” - This is the pattern
microagent replicates locally.</p>
<div class="centered">
<p><img src="screenshots/demo-project-docs.png" width="500"></p>
</div>
</section>
<section id="six-primitives-power-claude-projects" class="slide level1">
<h1>Six primitives power Claude Projects</h1>
<table>
<colgroup>
<col style="width: 29%" />
<col style="width: 32%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr>
<th>Primitive</th>
<th>What it is</th>
<th>How I use it</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Conversation</strong></td>
<td>Permanent chat, context not retained</td>
<td>Daily check-ins</td>
</tr>
<tr>
<td><strong>Artifact</strong></td>
<td>Generated content</td>
<td>Summaries, retros</td>
</tr>
<tr>
<td><strong>File</strong></td>
<td>Project document</td>
<td>Persistent context</td>
</tr>
<tr>
<td><strong>Skill</strong></td>
<td>On-demand instructions (.zip)</td>
<td>Structured workflows, “progressive disclosure” functions</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>30 slots (200 chars each), agent/UI editable</td>
<td>passively using, but finding it lacking</td>
</tr>
<tr>
<td><strong>Instructions</strong></td>
<td>System prompt</td>
<td>(skipped to simplify, we already have a lot of files that give
general instructions)</td>
</tr>
</tbody>
</table>
<p>The workflow uses <strong>conversations → artifacts →
files</strong>.</p>
</section>
<section
id="microagent-replicates-claude-projects-locally-as-simply-as-possible"
class="slide level1">
<h1>Microagent replicates Claude Projects locally, as simply as
possible</h1>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 45%" />
<col style="width: 32%" />
</colgroup>
<thead>
<tr>
<th>Aspect</th>
<th>Claude-as-Coach</th>
<th>Microagent</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Platform</strong></td>
<td>Claude.ai (web)</td>
<td>Local CLI</td>
</tr>
<tr>
<td><strong>Code required</strong></td>
<td>None</td>
<td>~1500 lines Python</td>
</tr>
<tr>
<td><strong>Context loading</strong></td>
<td>All files always loaded</td>
<td>Flexible (can eval read-file vs packed-context)</td>
</tr>
<tr>
<td><strong>Skill invocation</strong></td>
<td>Project knowledge</td>
<td><code>find_skill</code> tool</td>
</tr>
<tr>
<td><strong>Persistence</strong></td>
<td>Artifacts (manual)</td>
<td><code>write_file</code> tool</td>
</tr>
<tr>
<td><strong>Token budget</strong></td>
<td>~60-70k pre-loaded</td>
<td>~500 base + on demand</td>
</tr>
</tbody>
</table>
</section>
<section id="skills-are-markdown-instructions-that-act-like-functions"
class="slide level1">
<h1>Skills are markdown instructions that act like functions</h1>
<p>They can be used to keep long, semi-frequently used instructions from
chewing up context window.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">name</span><span class="kw">:</span><span class="at"> daily-summary-base</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">description</span><span class="kw">:</span><span class="at"> Use when user says &quot;daily summary&quot;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Daily Summary Generator</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">## Process</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="at">1. Verify current date (tool call)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="at">2. Ask which date to summarize</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="at">3. Generate structured markdown...</span></span></code></pre></div>
<p><strong>Claude matches triggers → loads skill → executes
process</strong></p>
<p><em>Replicated in microagent with <code>find_skill</code>
tool.</em></p>
</section>
<section
id="summary-of-summaries-is-all-you-need-for-context-management"
class="slide level1">
<h1>Summary-of-summaries is all you need for context management</h1>
<pre><code>Sun Mon Tue Wed Thu Fri Sat
 │   │   │   │   │   │   │
 └───┴───┴───┼───┴───┴───┘
             ▼
      Weekly Retro (1 doc)</code></pre>
<pre><code>Week 1 ─┐
Week 2 ─┼──► Monthly Retro (1 doc)
Week 3 ─┤
Week 4 ─┘</code></pre>
<p><strong>Summary-of-summaries is all you need</strong> (for this
workflow)</p>
<p>Currently very HITL and manual:</p>
<ul>
<li><strong>Good:</strong> Forces reflection, improves summary quality,
human stays engaged with goals</li>
<li><strong>Bad:</strong> Adding/removing files manually is
friction</li>
</ul>
</section>
<section
id="daily-use-of-claude-as-coach-prototype-since-october-2025-validated-the-workflows"
class="slide level1">
<h1>Daily use of Claude-As-Coach Prototype since October 2025 validated
the workflows</h1>
<p><strong>Benefits:</strong></p>
<ul>
<li>Zero infrastructure - just markdown files</li>
<li>Cross-device (phone + laptop) - full stack is built and
workable</li>
<li>Skills guide workflows effectively</li>
<li>Fractal compression via regular retrospective sessions prevents
unbounded growth</li>
<li>Frontier inferencing for a reasonable subscription price</li>
</ul>
<p><strong>Daily use since October 2025</strong></p>
</section>
<section id="the-prototype-hit-its-limits" class="slide level1">
<h1>The prototype hit its limits</h1>
<p><strong>Workflow friction (why move local):</strong></p>
<ul>
<li><strong>No analysis:</strong> Conversation export lacks project join
table, and is missing a lot of useful metadata - so even trying to
analyze an export is awkward</li>
<li><strong>No ownership:</strong> Data lives on Anthropic’s
servers</li>
<li><strong>No automation:</strong> Daily summary requires manual
artifact workflow, retrospectives and rolled up files require manual
addition to and removal from project files - this can definitely be
automated</li>
<li><strong>No skill isolation:</strong> Very challenging to iterate on
how the skills impact quality of daily summaries and retro
summary-of-summaries</li>
<li><strong>No real evals:</strong> Vibes-based only, not worth
automating against web UI, Opus and Sonnet are great, Haiku less so -
limited model selection</li>
</ul>
<p><strong>Goal:</strong> Same workflow, any model, local-first, with
eval capability</p>
</section>
<section id="a-minimal-agent-needs-just-500-lines-and-5-tools"
class="slide level1">
<h1>A minimal agent needs just ~500 lines and 5 tools</h1>
<p><strong>Question:</strong> What’s the smallest thing that replicates
claude-as-coach project in claude.ai?</p>
<p><strong>Answer:</strong></p>
<ul>
<li>REPL with streaming as UI</li>
<li>4 file tools (<code>get_current_date</code>,
<code>list_files</code>, <code>read_file</code>,
<code>write_file</code>)</li>
<li>Skill loading (<code>find_skill</code>)</li>
<li>Sandboxed project files in a directory on local disk</li>
<li>Conversation logging (JSONL)</li>
</ul>
<p>Started at ~500 lines. “Nanochat” philosophy - simple pedagogic
demonstration of foundational ideas first</p>
</section>
<section id="microagent-architecture" class="slide level1">
<h1>Microagent Architecture</h1>
<h2 id="how-the-local-agent-works">How the local agent works</h2>
</section>
<section id="the-core-loop-is-intentionally-very-simple"
class="slide level1">
<h1>The core loop is intentionally very simple</h1>
<pre><code>User Input → REPL → Format Message → OpenRouter API
                                          ↓
                                    Model Response
                                          ↓
                              Tool Calls? → Execute Tools
                                          ↓
                                    Display Response
                                          ↓
                                Save to JSONL log</code></pre>
</section>
<section
id="four-file-tools-ground-the-model-in-reality-and-let-it-manage-long-term-memory-documents"
class="slide level1">
<h1>Four file tools ground the model in reality and let it manage “long
term memory” documents</h1>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>get_current_date</code></td>
<td>Ground truth for temporal boundaries</td>
</tr>
<tr>
<td><code>list_files</code></td>
<td>Discovery of summaries, instructions</td>
</tr>
<tr>
<td><code>read_file</code></td>
<td>Load content (summaries, skills)</td>
</tr>
<tr>
<td><code>write_file</code></td>
<td>Save artifacts (summaries, logs)</td>
</tr>
</tbody>
</table>
<p><strong>Key insights:</strong></p>
<ul>
<li>LLMs are bad at dates and need ground truth (this also lets us
provide dates to eval scenarios).</li>
<li>File system provides a simple and effective place to manage
context.</li>
</ul>
</section>
<section id="three-primitives-skills-documents-conversations"
class="slide level1">
<h1>Three primitives: Skills, Documents, Conversations</h1>
<table>
<thead>
<tr>
<th>Primitive</th>
<th>Role</th>
<th>Analog</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Skills</strong></td>
<td>Functions</td>
<td>Executable instructions</td>
</tr>
<tr>
<td><strong>Documents</strong></td>
<td>Data</td>
<td>Summaries, context, state</td>
</tr>
<tr>
<td><strong>Conversations</strong></td>
<td>Interface</td>
<td>REPL, data entry, analysis</td>
</tr>
</tbody>
</table>
<p><strong>Skills</strong> define <em>what</em> to do.
<strong>Documents</strong> provide <em>context</em>.
<strong>Conversations</strong> are <em>how</em> users interact.</p>
<p>Together: A simple knowledge management system with no hidden
state.</p>
</section>
<section id="skills-define-what-to-dosame-format-as-claude.ai"
class="slide level1">
<h1>Skills define what to do—same format as Claude.ai</h1>
<p>Same skill format as Claude.ai, but loaded locally.</p>
<pre><code>skills/
├── daily-summary/SKILL.md
├── morning-routine/SKILL.md
└── weekly-retro/SKILL.md</code></pre>
<p>Triggers: “daily summary”, “good morning”, “weekly retro”</p>
</section>
<section id="documents-provide-lossy-contextlike-human-memory"
class="slide level1">
<h1>Documents provide lossy context—like human memory</h1>
<div class="smaller">
<pre><code>documents/
├── Project-Goals.md                      # Always-present context
├── Monthly-Rollup-2025-October.md        # Compressed monthly summary
├── Summary-2025-12-08-Monday-W9-D1.md    # Recent daily summaries
├── Summary-2025-12-10-Wednesday-W9-D2.md
├── Weekly-Plan-2025-12-08-to-14-W9.md    # Current week&#39;s plan
├── Weekly-Retro-2025-11-24-to-30-W7.md   # Past retros (compressed)
└── Weekly-Retro-2025-12-08-to-14-W9.md</code></pre>
</div>
<p><strong>Lossy like human memory:</strong> “I don’t remember what I
ate for breakfast on Dec 3rd, but I know if I was usually eating
breakfast or skipping it.”</p>
<p>Forget extra details, hold important trends. Human and agent
readable.</p>
</section>
<section
id="every-api-call-and-response-is-logged-to-jsonltransparent-and-replayable"
class="slide level1">
<h1>Every API call and response is logged to JSONL—transparent and
replayable</h1>
<p>JSONL append-only logs, one file per day.</p>
<pre><code>conversations/
└── 2025-01-09-Thursday.jsonl</code></pre>
<p>Every message, tool call, and response is logged.</p>
<p>Transparent. Debuggable. Replayable.</p>
</section>
<section
id="ownership-over-agent-layer-enables-ab-testing-of-agent-approachesmodelsproviders"
class="slide level1">
<h1>Ownership over agent layer enables A/B testing of agent
approaches/models/providers</h1>
<p>If we’re not locked to Claude, which models work?</p>
<p>We explored frontier open weight models large and small.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>Tool-calling support</li>
<li>Reliable tool call file operations calls</li>
<li>Reasonable summary output</li>
<li>Reasonable cost</li>
</ul>
<p>Enter: The Eval System</p>
</section>
<section id="the-eval-system" class="slide level1">
<h1>The Eval System</h1>
<h2
id="infrastructure-to-compare-models-skills-and-agent-design">Infrastructure
to compare models, skills, and agent design</h2>
</section>
<section
id="each-layer-can-change-independentlyisolation-enables-iteration"
class="slide level1">
<h1>Each layer can change independently—isolation enables iteration</h1>
<pre><code>┌─────────────────────────────────────────────────────┐
│             EVAL RUNNER + EVAL JUDGE                │
│  eval_runner.py + subagents (eval orchestration)    │
├─────────────────────────────────────────────────────┤
│                   MICROAGENT                        │
│  microagent.py (tools, message handling)            │
├─────────────────────────────────────────────────────┤
│                    SKILLS                           │
│  SKILL.md files (workflow instructions)             │
├─────────────────────────────────────────────────────┤
│                   SCENARIOS                         │
│  fixtures, rubrics, expected behaviors              │
└─────────────────────────────────────────────────────┘
                         ↓
              MODEL/PROVIDER UNDER TEST</code></pre>
<p>Each layer can change independently. Changes to any layer will
require rerunning the eval stack.</p>
</section>
<section id="agent-development-follows-test-driven-patterns-using-evals"
class="slide level1">
<h1>Agent development follows test-driven patterns using evals</h1>
<p>Same discipline, different domain:</p>
<table>
<thead>
<tr>
<th>TDD</th>
<th>Agent Development</th>
</tr>
</thead>
<tbody>
<tr>
<td>Test harness</td>
<td>eval_runner</td>
</tr>
<tr>
<td>“Code” under test</td>
<td>Skills.md + microagent.py + …</td>
</tr>
<tr>
<td>Test fixtures</td>
<td>Documents, context</td>
</tr>
<tr>
<td>Test isolation</td>
<td>Provider locking</td>
</tr>
<tr>
<td>Assertions</td>
<td>Rubric criteria</td>
</tr>
<tr>
<td>Test runner</td>
<td>eval-runner-sonnet</td>
</tr>
<tr>
<td>Test judge</td>
<td>eval-judge-sonnet</td>
</tr>
</tbody>
</table>
<p><strong>The cycle:</strong></p>
<p>Write scenario → Run against model → Judge results → Tune skills →
Repeat</p>
<p><em>“Behavior Driven Development” becomes “Eval Driven
Development”</em></p>
</section>
<section id="harder-scenarios-differentiate-models-better"
class="slide level1">
<h1>Harder scenarios differentiate models better</h1>
<p>Started simple, evolved to complex:</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>What it tests</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>daily-summary-v2</strong></td>
<td>Can model write a daily summary?</td>
</tr>
<tr>
<td><strong>morning-routine-v1</strong></td>
<td>Can model read a file?</td>
</tr>
<tr>
<td><strong>weekly-retro-v4</strong></td>
<td>Context loading + write (gold standard)</td>
</tr>
</tbody>
</table>
<p>Harder scenarios → better differentiation. Current gold standard:
<code>weekly-retro-v4</code></p>
<p><strong>Methodology:</strong> Run each eval 2x → pass/pass,
pass/fail, or fail/fail. Increases confidence in consistency.</p>
</section>
<section id="stage-1-simple-evals-validated-the-stack"
class="slide level1">
<h1>Stage 1: Simple evals validated the stack</h1>
<p><strong>First goal:</strong> Does the eval pipeline actually
work?</p>
<ul>
<li><code>daily-summary-v1</code> tested basic
<code>file_read</code>/<code>write_file</code> tools</li>
<li>Verified microagent.py + eval_runner.py integration</li>
<li>Red-green-refactor: write scenario → run against model → fix
implementation</li>
</ul>
<p><strong>Each iteration:</strong> ~$0.002, ~20 seconds</p>
</section>
<section id="stage-2-multi-file-scenarios-compared-strategies"
class="slide level1">
<h1>Stage 2: Multi-file scenarios compared strategies</h1>
<p><strong>Next question:</strong> How should the agent load
context?</p>
<ul>
<li><code>weekly-retro</code> needed multiple files → two approaches
emerged</li>
<li><strong>File-reading:</strong> Agent calls <code>read_file</code>
for each document</li>
<li><strong>Context-packing:</strong> Pre-load documents in system
prompt</li>
</ul>
<p>Evolution in <code>evals/archive/</code>: v1 → v2 (provider locking)
→ v3 (prompt placement) → v4 (gold standard)</p>
</section>
<section id="stage-3-confounds-drove-iteration" class="slide level1">
<h1>Stage 3: Confounds drove iteration</h1>
<p><strong>Unexpected result:</strong> “Read beats Context” (5/5 vs
2/5)</p>
<p>Same model, wildly different results. Why?</p>
<p><strong>Discovery:</strong> OpenRouter was routing to different
quantized backends</p>
<ul>
<li>Same model ID → different provider → different quality</li>
<li>FP4 vs FP8 vs full precision = different tool-calling
reliability</li>
</ul>
<p><strong>Solution:</strong> <code>--provider</code> flag for
reproducibility</p>
</section>
<section id="the-eval-stack-has-two-layers" class="slide level1">
<h1>The eval stack has two layers</h1>
<p><strong>The separation:</strong></p>
<table>
<colgroup>
<col style="width: 26%" />
<col style="width: 23%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr>
<th>Layer</th>
<th>What</th>
<th>Trust level</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>eval_report.py</strong></td>
<td>Token counts, cost, timing, tool sequence</td>
<td>Deterministic—same input, same output</td>
</tr>
<tr>
<td><strong>Judge agent</strong></td>
<td>Workflow quality, empathy, data accuracy</td>
<td>Non-deterministic—LLM applies RUBRIC.md</td>
</tr>
</tbody>
</table>
<p><strong>Why this matters:</strong></p>
<ul>
<li>Metrics never drift—rerun anytime, get same numbers</li>
<li>Judge criteria can evolve without re-running evals</li>
<li>Debugging: is it the model or the rubric?</li>
</ul>
<p><strong>CLI:</strong> <code>uv run eval-runner</code> →
<code>uv run eval-report</code> → (wip: eval-rollup)</p>
</section>
<section id="results-model-comparison" class="slide level1">
<h1>Results: Model Comparison</h1>
<h2 id="what-the-evals-revealed">What the evals revealed</h2>
</section>
<section id="three-layers-transform-jsonl-into-assessments"
class="slide level1">
<h1>Three layers transform JSONL into assessments</h1>
<pre><code>claude code orchestrator
eval-runner-sonnet ↓ eval_runner.py + microagent.py 
┌─────────────────────────────────────────────────────────┐
│  JSONL (raw log)                                        │
│  {&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;daily summary&quot;...}           │
│  {&quot;role&quot;:&quot;assistant&quot;,&quot;tool_calls&quot;:[{find_skill}]...}    │
└─────────────────────────────────────────────────────────┘
eval-judge-sonnet ↓ eval_report.py (deterministic)
┌─────────────────────────────────────────────────────────┐
│  conversation_report.txt                                │
│  Metrics + human-readable formatted transcript          │
└─────────────────────────────────────────────────────────┘
eval-judge-sonnet ↓ eval-judge-sonnet.md (non-deterministic)
┌─────────────────────────────────────────────────────────┐
│  evaluation-sonnet.md                                   │
│  Verdict, workflow checkboxes, quality ratings          │
└─────────────────────────────────────────────────────────┘</code></pre>
<p><strong>Deterministic metrics feed non-deterministic
judgment.</strong></p>
</section>
<section id="metrics-extraction-is-pure-pythonno-llm"
class="slide level1">
<h1>Metrics extraction is pure Python—no LLM</h1>
<p><strong>Deterministic extraction from JSONL</strong> - no LLM
needed:</p>
<pre><code>==================================================
METRICS SUMMARY
==================================================
Model: qwen/qwen3-235b-a22b-2507
Provider(s): DeepInfra (deepinfra/fp8)

Turn-by-Turn Breakdown:
| Turn | Provider | Tool Calls | Tokens | Status |
|------|----------|------------|--------|--------|
| 4 | DeepInfra | find_skill | 1808 | OK |
| 5 | DeepInfra | get_current_date | 3515 | OK |
| 7 | DeepInfra | write_file | 4720 | OK |

Turns: 8 | Tokens: 22,878 | Cost: $0.002
Tool Sequence: find_skill -&gt; get_current_date -&gt; write_file
Artifacts: [OK] Summary-2026-01-06-Tuesday-couch-to-5k-week1.md</code></pre>
<p>Plus full formatted transcript (human-readable, not raw JSON).</p>
</section>
<section id="the-judge-applies-rubric-to-score-1-5"
class="slide level1">
<h1>The judge applies rubric to score 1-5</h1>
<p><strong>Judge applies rubric</strong> to conversation report:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>**Verdict**: PASSED | **Score**: 5/5</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">## Skill Workflow</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Called find_skill(&quot;daily-summary-base&quot;)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Called get_current_date for date verification</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Called write_file to save summary</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conversation Quality: EXCELLENT</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tone: Encouraging without being preachy</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Empathy: Acknowledged vulnerability, connected to &quot;why&quot;</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Accuracy (8/8 facts captured)</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> 24 minutes total time</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> 3/10 feeling rating</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Kids (6 and 8) motivation</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Wife&#39;s &quot;two weeks&quot; advice</span></code></pre></div>
</section>
<section id="codebase-and-dev-stack" class="slide level1">
<h1>Codebase and Dev Stack</h1>
<h2 id="how-microagent-is-built">How microagent is built</h2>
</section>
<section id="lines-across-3-files-power-the-eval-tdd-stack"
class="slide level1">
<h1>~3,600 lines across 3 files power the eval TDD stack</h1>
<p><strong>The eval TDD stack:</strong></p>
<table>
<thead>
<tr>
<th>File</th>
<th>Lines</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td>microagent.py</td>
<td>1,546</td>
<td>Agent runtime (REPL, tools, skills)</td>
</tr>
<tr>
<td>eval_runner.py</td>
<td>836</td>
<td>Scenario execution (5 eval flows)</td>
</tr>
<tr>
<td>eval_report.py</td>
<td>1,242</td>
<td>Metrics extraction + report generation</td>
</tr>
</tbody>
</table>
<p>Plus unit tests for each, and a system test using
<code>nemotron:free</code> as a cheap safety net.</p>
</section>
<section
id="most-complexity-lives-in-user-interaction-and-scenario-runners"
class="slide level1">
<h1>Most complexity lives in user interaction and scenario runners</h1>
<p><strong>microagent.py (1,546 lines):</strong></p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Lines</th>
<th>What</th>
</tr>
</thead>
<tbody>
<tr>
<td>User interaction</td>
<td>~530</td>
<td>REPL + MessageHandler</td>
</tr>
<tr>
<td>Tools + Skills</td>
<td>~280</td>
<td>4 file tools, skill loading</td>
</tr>
<tr>
<td>Observability</td>
<td>~250</td>
<td>JSONL logging, token counting</td>
</tr>
<tr>
<td>Infrastructure</td>
<td>~480</td>
<td>API, streaming, errors, types</td>
</tr>
</tbody>
</table>
<p><strong>eval_runner.py (836 lines):</strong></p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Lines</th>
<th>What</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scenario runners</td>
<td>~370</td>
<td>5 eval conversation flows</td>
</tr>
<tr>
<td>Provider discovery</td>
<td>~170</td>
<td>OpenRouter API, validation</td>
</tr>
<tr>
<td>Config + CLI</td>
<td>~170</td>
<td>Setup, dispatch, argparse</td>
</tr>
<tr>
<td>Report + fixtures</td>
<td>~130</td>
<td>Output, Rob snippets</td>
</tr>
</tbody>
</table>
</section>
<section
id="eval_report.py-does-the-heavy-lifting-between-running-and-judging"
class="slide level1">
<h1>eval_report.py does the heavy lifting between running and
judging</h1>
<p><strong>eval_report.py (1,242 lines):</strong></p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Lines</th>
<th>What</th>
</tr>
</thead>
<tbody>
<tr>
<td>Metrics extraction</td>
<td>~225</td>
<td>Tokens, cost, timing, per-turn</td>
</tr>
<tr>
<td>Workflow signals</td>
<td>~145</td>
<td>Tool sequence, confirmation detection</td>
</tr>
<tr>
<td>Data accuracy</td>
<td>~65</td>
<td>Pattern matching (8 facts)</td>
</tr>
<tr>
<td>Template generation</td>
<td>~175</td>
<td>evaluation.md with backups</td>
</tr>
<tr>
<td>Display + formatting</td>
<td>~330</td>
<td>Pretty-print, markdown tables</td>
</tr>
<tr>
<td>CLI + orchestration</td>
<td>~165</td>
<td>Args, file I/O</td>
</tr>
</tbody>
</table>
<p><em>Deterministic metrics → Non-deterministic judge
assessment</em></p>
</section>
<section id="dev-stack" class="slide level1">
<h1>Dev Stack</h1>
<h2 id="modern-python-tooling-for-agent-development">Modern Python
tooling for agent development</h2>
</section>
<section id="five-tools-configured-in-one-pyproject.toml"
class="slide level1">
<h1>Five tools configured in one pyproject.toml</h1>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>uv</strong></td>
<td>Fast package manager + venv</td>
</tr>
<tr>
<td><strong>pytest</strong></td>
<td>Testing framework</td>
</tr>
<tr>
<td><strong>ruff</strong></td>
<td>Linting + formatting (replaces black, isort, flake8)</td>
</tr>
<tr>
<td><strong>mypy</strong></td>
<td>Type checking</td>
</tr>
<tr>
<td><strong>pyright</strong></td>
<td>Strict type checking (IDE integration)</td>
</tr>
</tbody>
</table>
<p>All configured in one file: <code>pyproject.toml</code></p>
</section>
<section id="uv-is-10-100x-faster-than-pipno-activation-needed"
class="slide level1">
<h1>uv is 10-100x faster than pip—no activation needed</h1>
<div class="sourceCode" id="cb12"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># One-time setup</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-LsSf</span> https://astral.sh/uv/install.sh <span class="kw">|</span> <span class="fu">sh</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Project setup (creates venv + installs deps)</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> sync</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run anything</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run microagent</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run pytest</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run ruff check .</span></code></pre></div>
<p><strong>10-100x faster</strong> than pip. Replaces pip, venv,
pip-tools.</p>
<p><strong>Key UX win:</strong> No <code>source venv/bin/activate</code>
before every command - helpful for humans and agents</p>
</section>
<section id="cli-tool-entry-points-are-safer-to-allow-list-for-agents"
class="slide level1">
<h1>CLI tool entry points are safer to allow-list for agents</h1>
<div class="sourceCode" id="cb13"><pre
class="sourceCode toml"><code class="sourceCode toml"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">[project.scripts]</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="dt">microagent</span> <span class="op">=</span> <span class="st">&quot;microagent:main&quot;</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="dt">eval-runner</span> <span class="op">=</span> <span class="st">&quot;eval_runner:main&quot;</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="dt">eval-report</span> <span class="op">=</span> <span class="st">&quot;eval_runner:report_main&quot;</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="dt">check-style</span> <span class="op">=</span> <span class="st">&quot;check_style:main&quot;</span></span></code></pre></div>
<p>Expose all CLI tools for the project:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run microagent        <span class="co"># Daily logging REPL</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run eval-runner       <span class="co"># Run model evaluations</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run eval-report       <span class="co"># Generate eval reports</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run check-style       <span class="co"># Custom style guide analyzer</span></span></code></pre></div>
<p><strong>Security insight:</strong> Entry points are safer to
allow-list than arbitrary <code>python</code> or
<code>uv run python</code> calls.</p>
</section>
<section id="dev-workflows" class="slide level1">
<h1>Dev Workflows</h1>
<h2 id="how-agent-assisted-development-works">How agent-assisted
development works</h2>
</section>
<section id="specialized-agents-handle-routine-tasks"
class="slide level1">
<h1>Specialized agents handle routine tasks</h1>
<p>Mapping complexity of task to appropriate cost model:
<code>.claude/agents/</code> contains specialized automation that claude
code can delegate to:</p>
<table>
<thead>
<tr>
<th>Agent</th>
<th>Model</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cladue Code</td>
<td>Opus 4.5</td>
<td>Orchestrator</td>
</tr>
<tr>
<td><code>ruff-fixer</code></td>
<td>Haiku 4.5</td>
<td>Run ruff, fix issues</td>
</tr>
<tr>
<td><code>mypy-error-fixer</code></td>
<td>Haiku 4.5</td>
<td>Run mypy, fix type errors</td>
</tr>
<tr>
<td><code>pyright-fixer</code></td>
<td>Haiku 4.5</td>
<td>Stricter type checking</td>
</tr>
<tr>
<td><code>pytest-runner</code></td>
<td>Haiku 4.5</td>
<td>Run tests, report results</td>
</tr>
<tr>
<td><code>eval-runner-*</code></td>
<td>Sonnet 4.5</td>
<td>Execute model evaluations</td>
</tr>
<tr>
<td><code>eval-judge-*</code></td>
<td>Sonnet 4.5</td>
<td>Assess eval results</td>
</tr>
</tbody>
</table>
<p><strong>Current:</strong> Agents can be asked to run manually.</p>
<p><strong>Future goal:</strong> Better automation of agent
invocation.</p>
</section>
<section
id="four-agents-run-sequentially-parallel-lintingfixing-causes-file-conflicts"
class="slide level1">
<h1>Four agents run sequentially — parallel linting/fixing causes file
conflicts</h1>
<p>Run in sequence (not parallel):</p>
<pre><code>1. style-guide-fixer  → Project-specific rules
         ↓
2. ruff-fixer         → Linting + formatting
         ↓
3. mypy-error-fixer   → Type checking
         ↓
4. pyright-fixer      → Strict type checking</code></pre>
<p><strong>Why sequential?</strong> Agents modify files. Parallel =
conflicts.</p>
<p><em><code>style-guide-fixer</code> calls <code>check_style.py</code>
(290 lines): AST-based keyword-only enforcement.</em> <em>Makes function
signatures legible for humans and agents.</em> * <code>pytest</code> to
run unit tests and one “happy path” system test</p>
</section>
<section id="backlog-lives-in-markdown-readable-by-both-human-and-agent"
class="slide level1">
<h1>Backlog lives in markdown — readable by both human and agent</h1>
<pre><code>docs/
├── features/              # Feature specs (FEATURE-*.md)
│   ├── FEATURE-eval-backfill.md
│   ├── FEATURE-thinking-models.md
│   └── README.md          # Feature pipeline
│
├── instructions/          # Workflow guides
│   ├── HOUSEKEEPING.md
│   └── SESSION-PLANNING.md
│
└── NEXT_SESSION.md        # What to work on next</code></pre>
<p>Claude Code reads these for context.</p>
<p><strong>Note:</strong> <code>instructions/</code> contains prompts
evolving into skills or subagents — a prototyping ground.</p>
</section>
<section
id="next-session-doc-lets-us-pick-a-path-based-on-timeenergy-available"
class="slide level1">
<h1>Next Session doc lets us pick a path based on time/energy
available</h1>
<p>Using the doc structure above, <code>docs/NEXT_SESSION.md</code>
provides capacity-matched paths:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">## Path A: High Capacity (60+ min)</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Implement new scenario</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Run full eval suite</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu">## Path B: Medium Capacity (30-45 min)</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fix specific issues</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Update documentation</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="fu">## Path C: Low Capacity (15-30 min)</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Review eval results</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Triage backlog</span></code></pre></div>
<p>Start each session by reading this file.</p>
</section>
<section id="what-we-learned" class="slide level1">
<h1>What We Learned</h1>
<h2 id="methodology-insights-from-iteration">Methodology insights from
iteration</h2>
</section>
<section
id="separate-deterministic-metrics-from-non-deterministic-judgment"
class="slide level1">
<h1>Separate deterministic metrics from non-deterministic judgment</h1>
<p><strong>The key architectural insight:</strong></p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 48%" />
<col style="width: 32%" />
</colgroup>
<thead>
<tr>
<th>Layer</th>
<th>What it measures</th>
<th>Properties</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Metrics</strong> (eval_report.py)</td>
<td>Tokens, cost, tool sequence</td>
<td>Deterministic—same input, same output</td>
</tr>
<tr>
<td><strong>Judgment</strong> (eval-judge agent)</td>
<td>Quality, empathy, accuracy</td>
<td>Non-deterministic—LLM applies rubric</td>
</tr>
</tbody>
</table>
<p><strong>Why this matters:</strong></p>
<ul>
<li>Metrics never drift—rerun anytime, get same numbers</li>
<li>Judgment criteria can evolve without re-running evals</li>
<li>Debugging: is it the model behavior or the rubric wording?</li>
</ul>
</section>
<section id="two-failure-categories-require-different-fixes"
class="slide level1">
<h1>Two failure categories require different fixes</h1>
<p><strong>Execution failures</strong> (tool-calling mechanics):</p>
<ul>
<li>Printing tool calls as text instead of executing</li>
<li>Never calling required tools (e.g., write_file)</li>
<li>Exposing internal reasoning in output</li>
</ul>
<p><strong>Comprehension failures</strong> (understanding despite
correct tools):</p>
<ul>
<li>Ignoring file content after successfully reading it</li>
<li>Role confusion (coaching instead of logging)</li>
<li>Constraint violations (writing 2 files when told “exactly ONE”)</li>
</ul>
<p><strong>Key insight:</strong> Both look like “the model failed” but
require different fixes.</p>
</section>
<section id="provider-routing-hides-confoundslock-early"
class="slide level1">
<h1>Provider routing hides confounds—lock early</h1>
<p>OpenRouter routes to multiple backends. <strong>Same model ID,
different behavior.</strong></p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Lock provider for reproducibility</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> run eval-runner <span class="at">--model</span> moonshotai/kimi-k2 <span class="at">--provider</span> deepinfra</span></code></pre></div>
<p><strong>What we found through iteration (v1 → v4):</strong></p>
<ul>
<li>Router switching between quantized versions</li>
<li>FP4 quantization breaking JSON tool call format</li>
<li>Same model scoring 1/5 or 5/5 depending on backend</li>
</ul>
<p><strong>Key learning:</strong> Not always the model—often the
deployment.</p>
<p><em>See also: <a
href="https://github.com/MoonshotAI/K2-Vendor-Verifier">K2-Vendor-Verifier</a>
— MoonshotAI tracks this across providers.</em></p>
</section>
<section id="open-questions-were-still-investigating"
class="slide level1">
<h1>Open questions we’re still investigating</h1>
<ul>
<li><p><strong>Tool order:</strong> Is canonical sequence necessary, or
are variations acceptable?</p></li>
<li><p><strong>Empathy variance:</strong> What enables empathetic vs
transactional responses? (Same task, completely different conversation
quality)</p></li>
<li><p><strong>Eval staleness:</strong> When do evals become stale as
implementation evolves?</p></li>
<li><p><strong>Context vs read:</strong> When does pre-loading context
outperform file reading?</p></li>
</ul>
</section>
<section id="immediate-next-steps" class="slide level1">
<h1>Immediate next steps</h1>
<table>
<colgroup>
<col style="width: 55%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr>
<th>Priority</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Provider-locked reruns</strong></td>
<td>Apples-to-apples comparison across models</td>
</tr>
<tr>
<td><strong>eval-rollup agent</strong></td>
<td>Surface insights faster (currently 30-60 min manual per
scenario)</td>
</tr>
<tr>
<td><strong>Web UI</strong></td>
<td>Better visibility into eval results</td>
</tr>
</tbody>
</table>
</section>
<section id="future-directions" class="slide level1">
<h1>Future directions</h1>
<p><strong>Scenario expansion:</strong></p>
<ul>
<li>Multi-turn evals (longer conversations)</li>
<li>Weekly planning scenario (FSM-based, flexible when agent goes “off
script”)</li>
<li>Week 2+ context accumulation tests</li>
</ul>
<p><strong>Thinking models:</strong></p>
<ul>
<li><code>--reasoning-effort</code> parameter tuning</li>
<li>Provider-locked runs needed first</li>
</ul>
<p><strong>Agent-managed memory:</strong></p>
<ul>
<li>Experiment with agent managing document lifecycle</li>
<li>When to compress, archive, or surface context?</li>
</ul>
</section>
<section id="three-lessons-for-ai-engineers-building-agents"
class="slide level1">
<h1>Three lessons for AI engineers building agents</h1>
<p>For AI engineers building agent systems:</p>
<ol type="1">
<li><p><strong>Evals-as-TDD works</strong> - Test models like you test
code. Red → Green → Refactor. Build scenarios that isolate the behavior
you care about.</p></li>
<li><p><strong>Provider routing hides confounds</strong> - Same model
ID, different backends = 3x quality variance. Lock your provider early
for reproducibility.</p></li>
<li><p><strong>Open weight models surprised us</strong> - DeepSeek,
Qwen, GLM, Kimi, and Nemotron all handled our scenarios effectively. We
found rough edges (especially when quantized), but they were
surprisingly capable for their sizes and prices.</p></li>
</ol>
</section>
<section id="models-we-tested-daily-summary-v2" class="slide level1">
<h1>Models we tested (daily-summary-v2)</h1>
<div class="smaller">
<table>
<thead>
<tr>
<th>Model</th>
<th>Provider</th>
<th>Results</th>
<th>Hot take</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>GLM-4.7</strong></td>
<td>DeepInfra</td>
<td>5/5, 5/5</td>
<td>Consistent</td>
</tr>
<tr>
<td><strong>DeepSeek v3.2</strong></td>
<td>DeepInfra</td>
<td>5/5, 5/5</td>
<td>Consistent</td>
</tr>
<tr>
<td><strong>Qwen3-235b</strong></td>
<td>DeepInfra</td>
<td>5/5</td>
<td>Great EQ</td>
</tr>
<tr>
<td><strong>MiniMax m2.1</strong></td>
<td>Minimax</td>
<td>4/5, 5/5</td>
<td>Solid</td>
</tr>
<tr>
<td><strong>Kimi K2-0905</strong></td>
<td>Groq</td>
<td>4/5, 5/5</td>
<td>Fast, good tone*</td>
</tr>
<tr>
<td><strong>Nemotron</strong></td>
<td>DeepInfra</td>
<td>4/5, 4/5</td>
<td>Reliable</td>
</tr>
<tr>
<td><strong>Nemotron:free</strong></td>
<td>Nvidia</td>
<td>4/5, 2/5</td>
<td>Inconsistent</td>
</tr>
<tr>
<td><strong>GPT-OSS-120b</strong></td>
<td>Groq</td>
<td>2/5, 4/5</td>
<td>Needs investigation</td>
</tr>
</tbody>
</table>
<p><em>Simplest eval—will push harder on complex scenarios.</em></p>
<p><em>*See <a
href="https://github.com/MoonshotAI/K2-Vendor-Verifier">K2-Vendor-Verifier</a>
— MoonshotAI’s tool-calling benchmark.</em></p>
</div>
</section>
<section id="working-on-similar-problems-want-to-collaborate"
class="slide level1">
<h1>Working on similar problems? Want to collaborate?</h1>
<p><strong>Reach out if you’re working on:</strong></p>
<ul>
<li>Model evaluation infrastructure</li>
<li>Skills-as-programs patterns</li>
<li>Long-term memory / summary-of-summaries</li>
<li>Agent orchestration with subagents</li>
</ul>
<p><strong>Find me:</strong> <a
href="https://discord.com/channels/822583790773862470/1209303473263485011">Latent
Space Discord - AI In Action channel</a></p>
<p>Open to working on open source portfolio projects, or consulting
projects together 😁</p>
<p><em>Repo is not open source yet - still iterating and cleaning it up.
Happy to share learnings and compare notes and do deeper code walkthrus
1on1.</em></p>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^5/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^5/plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^5/plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^5/plugin/zoom/zoom.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: true,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // reveal.js plugins
        plugins: [
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
